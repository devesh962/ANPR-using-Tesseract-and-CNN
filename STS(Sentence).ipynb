{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STS(Sentence).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1V-hdcBEzd0E8WLmHTvH5tVz1gLcfl2W4",
      "authorship_tag": "ABX9TyOjYGH/f8zmcJAS3CA+PuUe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qVItZtiLbn7",
        "colab_type": "text"
      },
      "source": [
        "# **Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAfe3jbaYgL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import logging\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxIts9PaYgH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJE0L2WL39hU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv(\"/content/drive/My Drive/Precily/Text_Similarity_Dataset.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTSfn4EY4Y6X",
        "colab_type": "code",
        "outputId": "70e15527-e48a-4a4a-e531-811c76df7dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train,test = train_test_split(data, random_state=30, test_size = 0.2)\n",
        "X_train = train.iloc[:,1:3]\n",
        "X_test = test.iloc[:,1:3]\n",
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3218, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNwHmPR3HGSU",
        "colab_type": "code",
        "outputId": "d3f1a767-523d-4338-9b17-31f5e3000b15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "X_test.head"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                   text1                                              text2\n",
              "3661  royal couple watch nation s mood prince charle...  firefox browser takes on microsoft microsoft s...\n",
              "307   young debut cut short by ginepri fifteen-year-...  top gig award for scissor sisters new york ban...\n",
              "1670  kennedy predicts bigger turnout voters   pent ...  straw backs ending china embargo uk foreign se...\n",
              "2268  howard s unfinished business  he s not finishe...  custody death rate  shocks  mps deaths in cust...\n",
              "1319  bafta to hand out movie honours movie stars fr...  ireland surge past scots ireland maintained th...\n",
              "...                                                 ...                                                ...\n",
              "3351  wenger offers mutu hope arsenal boss arsene we...  baghdad blogger on big screen a film based on ...\n",
              "999   firefox browser takes on microsoft microsoft s...  gamers snap up new sony psp gamers have bought...\n",
              "1752  oscars steer clear of controversy the oscars n...  applegate s charity show closes us musical swe...\n",
              "2874  italy 8-38 wales wales secured their first awa...  davenport hits out at wimbledon world number o...\n",
              "704   microsoft makes anti-piracy move microsoft say...  wilkinson to lead england fly-half jonny wilki...\n",
              "\n",
              "[805 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi0HRuF4YgEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_text():\n",
        "    \"\"\"\n",
        "    Extract questions for making word2vec model.\n",
        "    \"\"\"\n",
        "    df1 = X_train\n",
        "    df2 = X_test\n",
        "\n",
        "    for dataset in [df1, df2]:\n",
        "        for i, row in dataset.iterrows():\n",
        "            if i != 0 and i % 1000 == 0:\n",
        "                logging.info(\"read {0} sentences\".format(i))\n",
        "\n",
        "            if row['text1']:\n",
        "                yield gensim.utils.simple_preprocess(row['text1'])\n",
        "            if row['text2']:\n",
        "                yield gensim.utils.simple_preprocess(row['text2'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pPWggZNYf4I",
        "colab_type": "code",
        "outputId": "ae5c7c22-2b47-4710-80ce-e55479d9f5fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "documents = list(extract_text())\n",
        "logging.info(\"Done reading data file\")\n",
        "\n",
        "model = gensim.models.Word2Vec(documents, size=300)\n",
        "model.train(documents, total_examples=len(documents), epochs=10)\n",
        "model.save(\"/content/Text_Pairs.w2v\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-02-19 15:46:52,868 : INFO : read 2000 sentences\n",
            "2020-02-19 15:46:53,235 : INFO : read 4000 sentences\n",
            "2020-02-19 15:46:54,658 : INFO : read 3000 sentences\n",
            "2020-02-19 15:46:55,202 : INFO : read 1000 sentences\n",
            "2020-02-19 15:46:56,315 : INFO : Done reading data file\n",
            "2020-02-19 15:46:56,316 : INFO : collecting all words and their counts\n",
            "2020-02-19 15:46:56,317 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2020-02-19 15:46:56,850 : INFO : collected 27820 word types from a corpus of 2978631 raw words and 8046 sentences\n",
            "2020-02-19 15:46:56,852 : INFO : Loading a fresh vocabulary\n",
            "2020-02-19 15:46:57,014 : INFO : effective_min_count=5 retains 18552 unique words (66% of original 27820, drops 9268)\n",
            "2020-02-19 15:46:57,015 : INFO : effective_min_count=5 leaves 2944874 word corpus (98% of original 2978631, drops 33757)\n",
            "2020-02-19 15:46:57,071 : INFO : deleting the raw counts dictionary of 27820 items\n",
            "2020-02-19 15:46:57,073 : INFO : sample=0.001 downsamples 42 most-common words\n",
            "2020-02-19 15:46:57,073 : INFO : downsampling leaves estimated 2331098 word corpus (79.2% of prior 2944874)\n",
            "2020-02-19 15:46:57,118 : INFO : estimated required memory for 18552 words and 300 dimensions: 53800800 bytes\n",
            "2020-02-19 15:46:57,119 : INFO : resetting layer weights\n",
            "2020-02-19 15:47:00,476 : INFO : training model with 3 workers on 18552 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2020-02-19 15:47:01,500 : INFO : EPOCH 1 - PROGRESS: at 18.10% examples, 409327 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:02,513 : INFO : EPOCH 1 - PROGRESS: at 36.97% examples, 417220 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:03,520 : INFO : EPOCH 1 - PROGRESS: at 55.31% examples, 421796 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:04,542 : INFO : EPOCH 1 - PROGRESS: at 73.17% examples, 419680 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:05,559 : INFO : EPOCH 1 - PROGRESS: at 91.80% examples, 421085 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:06,004 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:47:06,008 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:47:06,020 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:47:06,021 : INFO : EPOCH - 1 : training on 2978631 raw words (2330747 effective words) took 5.5s, 420821 effective words/s\n",
            "2020-02-19 15:47:07,043 : INFO : EPOCH 2 - PROGRESS: at 18.78% examples, 423807 words/s, in_qsize 6, out_qsize 1\n",
            "2020-02-19 15:47:08,070 : INFO : EPOCH 2 - PROGRESS: at 39.05% examples, 436137 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:09,077 : INFO : EPOCH 2 - PROGRESS: at 58.07% examples, 441354 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:10,083 : INFO : EPOCH 2 - PROGRESS: at 77.31% examples, 442263 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:11,083 : INFO : EPOCH 2 - PROGRESS: at 96.05% examples, 442000 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:11,291 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:47:11,295 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:47:11,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:47:11,301 : INFO : EPOCH - 2 : training on 2978631 raw words (2329995 effective words) took 5.3s, 441567 effective words/s\n",
            "2020-02-19 15:47:12,320 : INFO : EPOCH 3 - PROGRESS: at 19.08% examples, 432362 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:13,328 : INFO : EPOCH 3 - PROGRESS: at 38.37% examples, 433225 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:14,330 : INFO : EPOCH 3 - PROGRESS: at 57.57% examples, 440350 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:15,331 : INFO : EPOCH 3 - PROGRESS: at 76.61% examples, 442043 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:16,348 : INFO : EPOCH 3 - PROGRESS: at 93.16% examples, 429858 words/s, in_qsize 4, out_qsize 1\n",
            "2020-02-19 15:47:16,673 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:47:16,675 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:47:16,675 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:47:16,679 : INFO : EPOCH - 3 : training on 2978631 raw words (2331464 effective words) took 5.4s, 433694 effective words/s\n",
            "2020-02-19 15:47:17,694 : INFO : EPOCH 4 - PROGRESS: at 19.49% examples, 443968 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:18,715 : INFO : EPOCH 4 - PROGRESS: at 38.65% examples, 435960 words/s, in_qsize 4, out_qsize 1\n",
            "2020-02-19 15:47:19,716 : INFO : EPOCH 4 - PROGRESS: at 57.73% examples, 442246 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:20,728 : INFO : EPOCH 4 - PROGRESS: at 77.31% examples, 444181 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:21,731 : INFO : EPOCH 4 - PROGRESS: at 96.37% examples, 444875 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:21,887 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:47:21,892 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:47:21,905 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:47:21,905 : INFO : EPOCH - 4 : training on 2978631 raw words (2330479 effective words) took 5.2s, 446539 effective words/s\n",
            "2020-02-19 15:47:22,911 : INFO : EPOCH 5 - PROGRESS: at 19.49% examples, 445816 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:23,941 : INFO : EPOCH 5 - PROGRESS: at 39.60% examples, 446629 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:24,960 : INFO : EPOCH 5 - PROGRESS: at 58.05% examples, 441722 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:25,979 : INFO : EPOCH 5 - PROGRESS: at 77.31% examples, 441124 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:26,983 : INFO : EPOCH 5 - PROGRESS: at 96.06% examples, 440886 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:27,171 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:47:27,178 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:47:27,180 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:47:27,181 : INFO : EPOCH - 5 : training on 2978631 raw words (2331484 effective words) took 5.3s, 442091 effective words/s\n",
            "2020-02-19 15:47:27,183 : INFO : training on a 14893155 raw words (11654169 effective words) took 26.7s, 436378 effective words/s\n",
            "2020-02-19 15:47:27,184 : WARNING : Effective 'alpha' higher than previous training cycles\n",
            "2020-02-19 15:47:27,185 : INFO : training model with 3 workers on 18552 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2020-02-19 15:47:28,201 : INFO : EPOCH 1 - PROGRESS: at 19.19% examples, 433653 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:29,205 : INFO : EPOCH 1 - PROGRESS: at 35.96% examples, 408131 words/s, in_qsize 6, out_qsize 0\n",
            "2020-02-19 15:47:30,223 : INFO : EPOCH 1 - PROGRESS: at 55.02% examples, 419349 words/s, in_qsize 3, out_qsize 2\n",
            "2020-02-19 15:47:31,248 : INFO : EPOCH 1 - PROGRESS: at 74.17% examples, 425129 words/s, in_qsize 4, out_qsize 1\n",
            "2020-02-19 15:47:32,277 : INFO : EPOCH 1 - PROGRESS: at 94.03% examples, 430448 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:32,557 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:47:32,563 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:47:32,568 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:47:32,569 : INFO : EPOCH - 1 : training on 2978631 raw words (2330605 effective words) took 5.4s, 433100 effective words/s\n",
            "2020-02-19 15:47:33,576 : INFO : EPOCH 2 - PROGRESS: at 19.49% examples, 445524 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:34,605 : INFO : EPOCH 2 - PROGRESS: at 39.05% examples, 439115 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:35,628 : INFO : EPOCH 2 - PROGRESS: at 58.41% examples, 443478 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:36,639 : INFO : EPOCH 2 - PROGRESS: at 76.57% examples, 437691 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:37,664 : INFO : EPOCH 2 - PROGRESS: at 96.37% examples, 440835 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:37,813 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:47:37,820 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:47:37,829 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:47:37,832 : INFO : EPOCH - 2 : training on 2978631 raw words (2330653 effective words) took 5.3s, 443111 effective words/s\n",
            "2020-02-19 15:47:38,878 : INFO : EPOCH 3 - PROGRESS: at 19.80% examples, 437638 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:39,882 : INFO : EPOCH 3 - PROGRESS: at 39.46% examples, 443636 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:40,906 : INFO : EPOCH 3 - PROGRESS: at 58.99% examples, 446665 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:41,913 : INFO : EPOCH 3 - PROGRESS: at 78.32% examples, 446295 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:42,930 : INFO : EPOCH 3 - PROGRESS: at 97.60% examples, 446774 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:43,021 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:47:43,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:47:43,046 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:47:43,048 : INFO : EPOCH - 3 : training on 2978631 raw words (2331124 effective words) took 5.2s, 447404 effective words/s\n",
            "2020-02-19 15:47:44,067 : INFO : EPOCH 4 - PROGRESS: at 19.49% examples, 441406 words/s, in_qsize 4, out_qsize 1\n",
            "2020-02-19 15:47:45,074 : INFO : EPOCH 4 - PROGRESS: at 39.31% examples, 445566 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:46,094 : INFO : EPOCH 4 - PROGRESS: at 58.40% examples, 445919 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:47,102 : INFO : EPOCH 4 - PROGRESS: at 77.31% examples, 443658 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:48,130 : INFO : EPOCH 4 - PROGRESS: at 96.73% examples, 443704 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:48,270 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:47:48,273 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:47:48,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:47:48,283 : INFO : EPOCH - 4 : training on 2978631 raw words (2330966 effective words) took 5.2s, 445769 effective words/s\n",
            "2020-02-19 15:47:49,307 : INFO : EPOCH 5 - PROGRESS: at 19.48% examples, 439564 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:50,309 : INFO : EPOCH 5 - PROGRESS: at 39.60% examples, 449331 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:51,326 : INFO : EPOCH 5 - PROGRESS: at 58.74% examples, 448732 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:52,329 : INFO : EPOCH 5 - PROGRESS: at 75.24% examples, 433095 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:53,339 : INFO : EPOCH 5 - PROGRESS: at 94.12% examples, 433970 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:53,610 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:47:53,632 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:47:53,637 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:47:53,638 : INFO : EPOCH - 5 : training on 2978631 raw words (2331090 effective words) took 5.3s, 435752 effective words/s\n",
            "2020-02-19 15:47:54,659 : INFO : EPOCH 6 - PROGRESS: at 19.08% examples, 433082 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:55,665 : INFO : EPOCH 6 - PROGRESS: at 39.60% examples, 449142 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:56,673 : INFO : EPOCH 6 - PROGRESS: at 58.41% examples, 447586 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:57,680 : INFO : EPOCH 6 - PROGRESS: at 77.31% examples, 445012 words/s, in_qsize 4, out_qsize 1\n",
            "2020-02-19 15:47:58,704 : INFO : EPOCH 6 - PROGRESS: at 97.37% examples, 448110 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:47:58,820 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:47:58,822 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:47:58,840 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:47:58,841 : INFO : EPOCH - 6 : training on 2978631 raw words (2331035 effective words) took 5.2s, 448522 effective words/s\n",
            "2020-02-19 15:47:59,880 : INFO : EPOCH 7 - PROGRESS: at 19.48% examples, 433141 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:00,889 : INFO : EPOCH 7 - PROGRESS: at 39.46% examples, 443556 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:01,903 : INFO : EPOCH 7 - PROGRESS: at 58.66% examples, 446050 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:02,915 : INFO : EPOCH 7 - PROGRESS: at 78.27% examples, 447055 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:03,920 : INFO : EPOCH 7 - PROGRESS: at 96.59% examples, 444063 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:04,075 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:48:04,080 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:48:04,084 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:48:04,085 : INFO : EPOCH - 7 : training on 2978631 raw words (2331194 effective words) took 5.2s, 445032 effective words/s\n",
            "2020-02-19 15:48:05,096 : INFO : EPOCH 8 - PROGRESS: at 19.08% examples, 436105 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:06,119 : INFO : EPOCH 8 - PROGRESS: at 38.37% examples, 431930 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:07,146 : INFO : EPOCH 8 - PROGRESS: at 58.07% examples, 440844 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:08,163 : INFO : EPOCH 8 - PROGRESS: at 77.69% examples, 442555 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:09,178 : INFO : EPOCH 8 - PROGRESS: at 96.73% examples, 442632 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:09,329 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:48:09,350 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:48:09,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:48:09,353 : INFO : EPOCH - 8 : training on 2978631 raw words (2331315 effective words) took 5.3s, 442769 effective words/s\n",
            "2020-02-19 15:48:10,370 : INFO : EPOCH 9 - PROGRESS: at 18.72% examples, 426776 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:11,388 : INFO : EPOCH 9 - PROGRESS: at 38.30% examples, 431958 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:12,417 : INFO : EPOCH 9 - PROGRESS: at 57.26% examples, 433644 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:13,432 : INFO : EPOCH 9 - PROGRESS: at 76.61% examples, 436966 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:14,438 : INFO : EPOCH 9 - PROGRESS: at 95.75% examples, 438856 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:14,648 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:48:14,657 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:48:14,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:48:14,671 : INFO : EPOCH - 9 : training on 2978631 raw words (2331456 effective words) took 5.3s, 438738 effective words/s\n",
            "2020-02-19 15:48:15,709 : INFO : EPOCH 10 - PROGRESS: at 18.78% examples, 418793 words/s, in_qsize 6, out_qsize 1\n",
            "2020-02-19 15:48:16,716 : INFO : EPOCH 10 - PROGRESS: at 36.71% examples, 411600 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:17,726 : INFO : EPOCH 10 - PROGRESS: at 55.95% examples, 425262 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:18,730 : INFO : EPOCH 10 - PROGRESS: at 74.88% examples, 430009 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:19,736 : INFO : EPOCH 10 - PROGRESS: at 94.49% examples, 434880 words/s, in_qsize 5, out_qsize 0\n",
            "2020-02-19 15:48:20,010 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-02-19 15:48:20,013 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-02-19 15:48:20,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-02-19 15:48:20,033 : INFO : EPOCH - 10 : training on 2978631 raw words (2331762 effective words) took 5.4s, 435424 effective words/s\n",
            "2020-02-19 15:48:20,033 : INFO : training on a 29786310 raw words (23311200 effective words) took 52.8s, 441101 effective words/s\n",
            "2020-02-19 15:48:20,036 : INFO : saving Word2Vec object under /content/Text_Pairs.w2v, separately None\n",
            "2020-02-19 15:48:20,037 : INFO : not storing attribute vectors_norm\n",
            "2020-02-19 15:48:20,040 : INFO : not storing attribute cum_table\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2020-02-19 15:48:20,594 : INFO : saved /content/Text_Pairs.w2v\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIZxtLXdLYGq",
        "colab_type": "text"
      },
      "source": [
        "# **UTIL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MmsbgpRYfzr",
        "colab_type": "code",
        "outputId": "652d6549-8b4a-496d-cf0a-e1cefa93e0ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import re\n",
        "\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import gensim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa14fK9dYfyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_word_list(text):\n",
        "    # Pre process and convert texts to a list of words\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP7U3bDCYfuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_w2v_embeddings(data, embedding_dim=300, empty_w2v=False):\n",
        "    vocabs = {}\n",
        "    vocabs_cnt = 0\n",
        "\n",
        "    vocabs_not_w2v = {}\n",
        "    vocabs_not_w2v_cnt = 0\n",
        "\n",
        "    # Stopwords\n",
        "    stops = set(stopwords.words('english'))\n",
        "\n",
        "    # Load word2vec\n",
        "    print(\"Loading word2vec model(it may takes 2-3 mins) ...\")\n",
        "\n",
        "    if empty_w2v:\n",
        "        word2vec = EmptyWord2Vec\n",
        "    else:\n",
        "        word2vec = KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "        # word2vec = gensim.models.word2vec.Word2Vec.load(\"./data/Quora-Question-Pairs.w2v\").wv\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        # Print the number of embedded sentences.\n",
        "        if index != 0 and index % 1000 == 0:\n",
        "            print(\"{:,} sentences embedded.\".format(index), flush=True)\n",
        "\n",
        "        # Iterate through the text of both questions of the row\n",
        "        for text in ['text1', 'text2']:\n",
        "\n",
        "            t2t = []  # t2t -> text numbers representation\n",
        "            for word in text_to_word_list(row[text]):\n",
        "                # Check for unwanted words\n",
        "                if word in stops:\n",
        "                    continue\n",
        "\n",
        "                # If a word is missing from word2vec model.\n",
        "                if word not in word2vec.vocab:\n",
        "                    if word not in vocabs_not_w2v:\n",
        "                        vocabs_not_w2v_cnt += 1\n",
        "                        vocabs_not_w2v[word] = 1\n",
        "\n",
        "                # If you have never seen a word, append it to vocab dictionary.\n",
        "                if word not in vocabs:\n",
        "                    vocabs_cnt += 1\n",
        "                    vocabs[word] = vocabs_cnt\n",
        "                    t2t.append(vocabs_cnt)\n",
        "                else:\n",
        "                    t2t.append(vocabs[word])\n",
        "\n",
        "            # Append question as number representation\n",
        "            data.at[index, text + '_n'] = t2t\n",
        "\n",
        "    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "    embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "    # Build the embedding matrix\n",
        "    for word, index in vocabs.items():\n",
        "        if word in word2vec.vocab:\n",
        "            embeddings[index] = word2vec.word_vec(word)\n",
        "    del word2vec\n",
        "\n",
        "    return data, embeddings\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xuZyi9BbM9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_and_zero_padding(df, max_seq_length):\n",
        "    # Split to dicts\n",
        "    X = {'left': df['text1_n'], 'right': df['text2_n']}\n",
        "\n",
        "    # Zero padding\n",
        "    for dataset, side in itertools.product([X], ['left', 'right']):\n",
        "        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAZTU7m1bM61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ManDist(Layer):\n",
        "    \"\"\"\n",
        "    Keras Custom Layer that calculates Manhattan Distance.\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize the layer, No need to include inputs parameter!\n",
        "    def __init__(self, **kwargs):\n",
        "        self.result = None\n",
        "        super(ManDist, self).__init__(**kwargs)\n",
        "\n",
        "    # input_shape will automatic collect input shapes to build layer\n",
        "    def build(self, input_shape):\n",
        "        super(ManDist, self).build(input_shape)\n",
        "\n",
        "    # This is where the layer's logic lives.\n",
        "    def call(self, x, **kwargs):\n",
        "        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n",
        "        return self.result\n",
        "\n",
        "    # return output shape\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return K.int_shape(self.result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d469Tj16bM3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmptyWord2Vec:\n",
        "    \"\"\"\n",
        "    Just for test use.\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    word_vec = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6R8kO_HLh4B",
        "colab_type": "text"
      },
      "source": [
        "# **TRAIN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXMKbuTcLxD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.models import Model, Sequential\n",
        "from tensorflow.python.keras.layers import Input, Embedding, LSTM, GRU, Conv1D, Conv2D, GlobalMaxPool1D, Dense, Dropout\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e65RSLHHLyro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load training set\n",
        "train_df = X_train\n",
        "for q in ['text1', 'text2']:\n",
        "    train_df[q + '_n'] = train_df[q]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPglxyoiMoA3",
        "colab_type": "code",
        "outputId": "b3400b4b-0044-47f1-e0da-0169843280e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WvVdk-fLyoz",
        "colab_type": "code",
        "outputId": "ba28bcf1-eabb-435d-9351-6530aff3c832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# Make word2vec embeddings\n",
        "embedding_dim = 300\n",
        "max_seq_length = 20\n",
        "use_w2v = True\n",
        "\n",
        "train_df, embeddings = make_w2v_embeddings(train_df, embedding_dim=embedding_dim, empty_w2v=not use_w2v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-02-19 16:14:39,139 : INFO : loading projection weights from /content/drive/My Drive/GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading word2vec model(it may takes 2-3 mins) ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2020-02-19 16:16:13,357 : INFO : loaded (3000000, 300) matrix from /content/drive/My Drive/GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2,000 sentences embedded.\n",
            "4,000 sentences embedded.\n",
            "3,000 sentences embedded.\n",
            "1,000 sentences embedded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itKNMy0sai2m",
        "colab_type": "text"
      },
      "source": [
        "# **Further code can be executed if labels would be given.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u03ka-ZOLymp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split to train validation\n",
        "validation_size = int(len(train_df) * 0.1)\n",
        "training_size = len(train_df) - validation_size\n",
        "\n",
        "X = train_df[['text1_n', 'text2_n']]\n",
        "Y = train_df['similarity_score']\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
        "\n",
        "X_train = split_and_zero_padding(X_train, max_seq_length)\n",
        "X_validation = split_and_zero_padding(X_validation, max_seq_length)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1TLwZn3Lyj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert labels to their numpy representations\n",
        "Y_train = Y_train.values\n",
        "Y_validation = Y_validation.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znrIpQsRYkEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure everything is ok\n",
        "assert X_train['left'].shape == X_train['right'].shape\n",
        "assert len(X_train['left']) == len(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFgLcjtoYkCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model variables\n",
        "gpus = 2\n",
        "batch_size = 400 * gpus\n",
        "n_epoch = 50\n",
        "n_hidden = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUP93uocYj-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the shared model\n",
        "x = Sequential()\n",
        "x.add(Embedding(len(embeddings), embedding_dim,\n",
        "                weights=[embeddings], input_shape=(max_seq_length,), trainable=False))\n",
        "\n",
        "x.add(LSTM(n_hidden))\n",
        "\n",
        "shared_model = x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcdYcSHBYj8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The visible layer\n",
        "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3R17lR-Yj3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pack it all up into a Manhattan Distance model\n",
        "malstm_distance = ManDist()([shared_model(left_input), shared_model(right_input)])\n",
        "model = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n",
        "\n",
        "if gpus >= 2:\n",
        "    # `multi_gpu_model()` is a so quite buggy. it breaks the saved model.\n",
        "    model = tf.keras.utils.multi_gpu_model(model, gpus=gpus)\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "model.summary()\n",
        "shared_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0hC_q9JY4gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start trainings\n",
        "training_start_time = time()\n",
        "malstm_trained = model.fit([X_train['left'], X_train['right']], Y_train,\n",
        "                           batch_size=batch_size, epochs=n_epoch,\n",
        "                           validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
        "training_end_time = time()\n",
        "print(\"Training time finished.\\n%d epochs in %12.2f\" % (n_epoch,\n",
        "                                                        training_end_time - training_start_time))\n",
        "\n",
        "model.save('/content/SiameseLSTM.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCFPUogFY4fS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot accuracy\n",
        "plt.subplot(211)\n",
        "plt.plot(malstm_trained.history['acc'])\n",
        "plt.plot(malstm_trained.history['val_acc'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(212)\n",
        "plt.plot(malstm_trained.history['loss'])\n",
        "plt.plot(malstm_trained.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.tight_layout(h_pad=1.0)\n",
        "plt.savefig('/content/history-graph.png')\n",
        "\n",
        "print(str(malstm_trained.history['val_acc'][-1])[:6] +\n",
        "      \"(max: \" + str(max(malstm_trained.history['val_acc']))[:6] + \")\")\n",
        "print(\"Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykDI20eYY4cK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjcL9XpPY4Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_CSV = X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbpXA1D1Y4Wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load training set\n",
        "test_df = TEST_CSV\n",
        "for q in ['text1', 'text2']:\n",
        "    test_df[q + '_n'] = test_df[q]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIEnfi17ZbHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make word2vec embeddings\n",
        "embedding_dim = 300\n",
        "max_seq_length = 20\n",
        "test_df, embeddings = make_w2v_embeddings(test_df, embedding_dim=embedding_dim, empty_w2v=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyHSZbZNZgkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split to dicts and append zero padding.\n",
        "X_test = split_and_zero_padding(test_df, max_seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK9Sbc_zZgbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure everything is ok\n",
        "assert X_test['left'].shape == X_test['right'].shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlO0nThbZlLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --\n",
        "\n",
        "model = tf.keras.models.load_model('/content/SiameseLSTM.h5', custom_objects={'ManDist': ManDist})\n",
        "model.summary()\n",
        "\n",
        "prediction = model.predict([X_test['left'], X_test['right']])\n",
        "print(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}